{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com/model/2362732/penisXL.IMcR.safetensors?X-Amz-Expires=86400&response-content-disposition=attachment%3B%20filename%3D%22Penis_XL.safetensors%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=e01358d793ad6966166af8b3064953ad/20240701/us-east-1/s3/aws4_request&X-Amz-Date=20240701T193521Z&X-Amz-SignedHeaders=host&X-Amz-Signature=70f679340841bc117c7bb391bcd4b588386511e78098e3bc8bda5774c64c8bfd\"\n",
        "output_file = \"/content/Fooocus/models/checkpoints/xlpenis.safetensors\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(url, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        with open(output_file, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(\"File downloaded successfully\")\n",
        "    else:\n",
        "        print(f\"Failed to download file, status code: {response.status_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during download: {str(e)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33sv6--HyE8o",
        "outputId": "ca9fbd4d-5e5c-46b5-d6c9-aa7765f7faef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "343655ac-bc1e-43fe-d06f-5b261bb23014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygit2==1.12.2 in /usr/local/lib/python3.10/dist-packages (1.12.2)\n",
            "Requirement already satisfied: cffi>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from pygit2==1.12.2) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.9.1->pygit2==1.12.2) (2.22)\n",
            "/content\n",
            "fatal: destination path 'Fooocus' already exists and is not an empty directory.\n",
            "/content/Fooocus\n",
            "Already up-to-date\n",
            "Update succeeded.\n",
            "[System ARGV] ['entry_with_update.py', '--share', '--always-high-vram']\n",
            "Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "Fooocus version: 2.4.3\n",
            "[Cleanup] Attempting to delete content of temp dir /tmp/fooocus\n",
            "[Cleanup] Cleanup successful\n",
            "Downloading: \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_offset_example-lora_1.0.safetensors\" to /content/Fooocus/models/loras/sd_xl_offset_example-lora_1.0.safetensors\n",
            "\n",
            "100% 47.3M/47.3M [00:00<00:00, 369MB/s]\n",
            "Total VRAM 15102 MB, total RAM 12979 MB\n",
            "Set vram state to: HIGH_VRAM\n",
            "Always offload VRAM\n",
            "Device: cuda:0 Tesla T4 : native\n",
            "VAE dtype: torch.float32\n",
            "Using pytorch cross attention\n",
            "2024-07-01 19:39:28.204160: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-01 19:39:28.204207: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-01 19:39:28.211117: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-01 19:39:30.254233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Refiner unloaded.\n",
            "Running on local URL:  http://127.0.0.1:7865\n",
            "IMPORTANT: You are using gradio version 3.41.2, however version 4.29.0 is available, please upgrade.\n",
            "--------\n",
            "Running on public URL: https://20fb36a26bd01c924b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "model_type EPS\n",
            "UNet ADM Dimension 2816\n",
            "Using pytorch attention in VAE\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "Using pytorch attention in VAE\n",
            "extra {'cond_stage_model.clip_l.logit_scale', 'cond_stage_model.clip_l.text_projection', 'cond_stage_model.clip_g.transformer.text_model.embeddings.position_ids'}\n",
            "loaded straight to GPU\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "Base model loaded: /content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors\n",
            "VAE loaded: None\n",
            "Request to load LoRAs [('sd_xl_offset_example-lora_1.0.safetensors', 0.1)] for model [/content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors].\n",
            "Loaded LoRA [/content/Fooocus/models/loras/sd_xl_offset_example-lora_1.0.safetensors] for UNet [/content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors] with 788 keys at weight 0.1.\n",
            "Fooocus V2 Expansion: Vocab with 642 words.\n",
            "Fooocus Expansion engine loaded for cuda:0, use_fp16 = True.\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.72 seconds\n",
            "Started worker with PID 6898\n",
            "App started successful. Use the app with http://127.0.0.1:7865/ or 127.0.0.1:7865 or https://20fb36a26bd01c924b.gradio.live\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] CFG = 4.0\n",
            "[Parameters] Seed = 5931927273656118798\n",
            "[Fooocus] Downloading upscale models ...\n",
            "[Fooocus] Downloading inpainter ...\n",
            "[Inpaint] Current inpaint model is /content/Fooocus/models/inpaint/inpaint_v26.fooocus.patch\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 24\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Synthetic Refiner Activated\n",
            "Synthetic Refiner Activated\n",
            "Request to load LoRAs [('xlpenis.safetensors', 0.1), ('/content/Fooocus/models/inpaint/inpaint_v26.fooocus.patch', 1.0)] for model [/content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors].\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Fooocus/modules/patch.py\", line 465, in loader\n",
            "    result = original_loader(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\", line 259, in load_file\n",
            "    with safe_open(filename, framework=\"pt\", device=device) as f:\n",
            "safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 998, in worker\n",
            "    handler(task)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 469, in handler\n",
            "    pipeline.refresh_everything(refiner_model_name=refiner_model_name, base_model_name=base_model_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Fooocus/modules/default_pipeline.py\", line 252, in refresh_everything\n",
            "    refresh_loras(loras, base_model_additional_loras=base_model_additional_loras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Fooocus/modules/default_pipeline.py\", line 139, in refresh_loras\n",
            "    model_base.refresh_loras(loras + base_model_additional_loras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/Fooocus/modules/core.py\", line 96, in refresh_loras\n",
            "    lora_unmatch = ldm_patched.modules.utils.load_torch_file(lora_filename, safe_load=False)\n",
            "  File \"/content/Fooocus/ldm_patched/modules/utils.py\", line 13, in load_torch_file\n",
            "    sd = safetensors.torch.load_file(ckpt, device=device.type)\n",
            "  File \"/content/Fooocus/modules/patch.py\", line 481, in loader\n",
            "    raise ValueError(exp)\n",
            "ValueError: Error while deserializing header: HeaderTooLarge\n",
            "File corrupted: /content/Fooocus/models/loras/xlpenis.safetensors \n",
            "Fooocus has tried to move the corrupted file to /content/Fooocus/models/loras/xlpenis.safetensors.corrupted \n",
            "You may try again now and Fooocus will download models again. \n",
            "\n",
            "Total time: 0.07 seconds\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] CFG = 4.0\n",
            "[Parameters] Seed = 7510216107491271502\n",
            "[Fooocus] Downloading upscale models ...\n",
            "[Fooocus] Downloading inpainter ...\n",
            "[Inpaint] Current inpaint model is /content/Fooocus/models/inpaint/inpaint_v26.fooocus.patch\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 24\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Synthetic Refiner Activated\n",
            "Synthetic Refiner Activated\n",
            "Request to load LoRAs [('sd_xl_offset_example-lora_1.0.safetensors', 0.1), ('xlpenis.safetensors', 1.0), ('/content/Fooocus/models/inpaint/inpaint_v26.fooocus.patch', 1.0)] for model [/content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors].\n",
            "Lora file not found: /content/Fooocus/models/loras/xlpenis.safetensors\n",
            "Loaded LoRA [/content/Fooocus/models/loras/sd_xl_offset_example-lora_1.0.safetensors] for UNet [/content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors] with 788 keys at weight 0.1.\n",
            "Loaded LoRA [/content/Fooocus/models/inpaint/inpaint_v26.fooocus.patch] for UNet [/content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors] with 960 keys at weight 1.0.\n",
            "Request to load LoRAs [('sd_xl_offset_example-lora_1.0.safetensors', 0.1), ('xlpenis.safetensors', 1.0)] for model [/content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors].\n",
            "Lora file not found: /content/Fooocus/models/loras/xlpenis.safetensors\n",
            "Loaded LoRA [/content/Fooocus/models/loras/sd_xl_offset_example-lora_1.0.safetensors] for UNet [/content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors] with 788 keys at weight 0.1.\n",
            "Requested to load SDXLClipModel\n",
            "Loading 1 new model\n",
            "unload clone 1\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.75 seconds\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Preparing Fooocus text #1 ...\n",
            "[Prompt Expansion] penis, perfect symmetry, vivid colors, highly detailed, intricate, elegant, professional atmosphere, magic, lush, best, cinematic, fine focus, dramatic, dynamic light, quality, shining, epic, stunning, gorgeous, creative, beautiful, confident, complex, color, background, illuminated, pristine, cute, attractive, artistic, deep detail, pretty, cool, awesome\n",
            "[Fooocus] Preparing Fooocus text #2 ...\n",
            "[Prompt Expansion] penis, intricate, elegant, highly detailed, wonderful quality, dramatic light, sharp focus, cinematic, fine composition, awesome color, dynamic, beautiful full, aesthetic, very inspirational, colorful, thought, fancy, elaborate, professional, winning, best, novel, modern, romantic, artistic, cool, epic, hopeful, inspired, vibrant, lovely, perfect, pretty\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.12 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Fooocus] Image processing ...\n",
            "Upscaling image with shape (475, 475, 3) ...\n",
            "[Fooocus] VAE Inpaint encoding ...\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.49 seconds\n",
            "[Fooocus] VAE encoding ...\n",
            "Final resolution is (768, 512), latent is (1024, 1024).\n",
            "[Parameters] Denoising Strength = 1\n",
            "[Parameters] Initial Latent shape: torch.Size([1, 4, 128, 128])\n",
            "Preparation time: 29.98 seconds\n",
            "[Fooocus] Preparing task 1/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.48 seconds\n",
            " 23% 7/30 [00:06<00:21,  1.09it/s]"
          ]
        }
      ],
      "source": [
        "!pip install pygit2==1.12.2\n",
        "%cd /content\n",
        "!git clone https://github.com/lllyasviel/Fooocus.git\n",
        "%cd /content/Fooocus\n",
        "!python entry_with_update.py --share --always-high-vram\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}